{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cPickle as pkl\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import custom models\n",
    "from model.Autoencoder import LSTMAuto\n",
    "from model.MIPnet import MIPnet\n",
    "from model.githubHandler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(model.MIPnet)\n",
    "reload(model.githubHandler)\n",
    "reload(model.Autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdata, udata, data, users, repos = pkl.load(open('data_structuring_150.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_encodings = {'MemberEvent': \"none\",\n",
    " 'PublicEvent': \"design\",\n",
    " 'PullRequestReviewCommentEvent': \"design\",\n",
    " 'ForkEvent': \"consume\",\n",
    " 'GollumEvent': \"design\",\n",
    " 'ReleaseEvent': \"none\",\n",
    " 'PullRequestEvent': \"content\",\n",
    " 'IssueCommentEvent': \"design\",\n",
    " 'PushEvent': \"content\",\n",
    " 'DeleteEvent': \"content\",\n",
    " 'CommitCommentEvent': \"design\",\n",
    " 'WatchEvent': \"consume\",\n",
    " 'IssuesEvent': \"design\",\n",
    " 'CreateEvent': \"content\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 16194 interactions\n"
     ]
    }
   ],
   "source": [
    "# Simulate time steps\n",
    "mip = MIPnet(users, repos)\n",
    "interactions_bucket1 = get_all_interactions_for_timebucket(1, data, label_encodings)\n",
    "mip.update_edges_for_time(interactions_bucket1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n",
      "found 16194 interactions\n",
      "34048 Edges between 26735 Nodes\n",
      "Bucket 2\n",
      "found 23973 interactions\n",
      "65740 Edges between 26735 Nodes\n",
      "Bucket 3\n",
      "found 31620 interactions\n",
      "95379 Edges between 26735 Nodes\n",
      "Bucket 4\n",
      "found 26499 interactions\n",
      "120711 Edges between 26735 Nodes\n",
      "Bucket 5\n",
      "found 38469 interactions\n",
      "172204 Edges between 26735 Nodes\n",
      "Bucket 6\n",
      "found 28763 interactions\n",
      "199121 Edges between 26735 Nodes\n",
      "Bucket 7\n",
      "found 26121 interactions\n",
      "226551 Edges between 26735 Nodes\n",
      "Bucket 8\n",
      "found 25313 interactions\n",
      "251684 Edges between 26735 Nodes\n"
     ]
    }
   ],
   "source": [
    "# simulate all time buckets\n",
    "mip = MIPnet(users, repos)\n",
    "for i in range(1,9):\n",
    "    print \"Bucket\", i\n",
    "    mip.update_edges_for_time(get_all_interactions_for_timebucket(i, data, label_encodings))\n",
    "    print len(mip.mip.edges()), \"Edges between\", len(users)+len(repos), \"Nodes\"\n",
    "    mip.decay_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ntype': 'content', 'weight': 9.850707420300003}\n",
      "{'ntype': 'design', 'weight': 3.4480423521000008}\n",
      "{'ntype': 'consume', 'weight': 0.7791456501000001}\n"
     ]
    }
   ],
   "source": [
    "for ednum,val in mip.mip[\"Angelfirenze\"][\"deadlyvipers/dojo_rules\"].iteritems():\n",
    "    print val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Section\n",
    "\n",
    "### adjusted MIP-DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How much should each weight be worth? Start with equal\n",
    "params_all =     {\"u-u\": 1, \"r-r\": 1, \"content\": 1, \"design\": 1, \"consume\": 1}\n",
    "params_content = {\"u-u\": 1, \"r-r\": 1, \"content\": 1, \"design\": 0, \"consume\": 0}\n",
    "params_design =  {\"u-u\": 1, \"r-r\": 1, \"content\": 0, \"design\": 1, \"consume\": 0}\n",
    "params_consume = {\"u-u\": 1, \"r-r\": 1, \"content\": 0, \"design\": 0, \"consume\": 1}\n",
    "params = [params_all, params_content, params_design, params_consume]\n",
    "# Ofras parameters\n",
    "alpha = .5\n",
    "beta = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11.68648641,   1.84795528,  10.24145771,   3.05212012])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adamicAdarProximity(mip.mip, \"Angelfirenze\", \"deadlyvipers/dojo_rules\", params)\n",
    "#TODO rescale both values or learn alpha/beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9185020100000003"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeUserInCommonWeight(mip.mip, \"Angelfirenze\", \"deadlyvipers/dojo_rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.8435424487595036, 0.0005984888157402558, array([ 11.68648641,   1.84795528,  10.24145771,   3.05212012]))\n"
     ]
    }
   ],
   "source": [
    "print computeDOI(mip, \"Angelfirenze\", \"deadlyvipers/dojo_rules\", params, alpha, beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 Number of sequence length for autoencoder\n"
     ]
    }
   ],
   "source": [
    "user_enc_name, x_ae, y_ae = constructAutoEncoderData(udata, label_encodings, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embsize = 5\n",
    "encodingsize = 5\n",
    "autoencoder = LSTMAuto(x_ae.shape[1], embsize, y_ae.shape[2], encodingsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  900/16576 [>.............................] - ETA: 286s - loss: 0.8276"
     ]
    }
   ],
   "source": [
    "# For full models, it is best to train for >5 epochs ~4.5 min per epoch\n",
    "autoencoder.train(x_ae, y_ae, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Predict all users hidden\n",
    "user_embs = autoencoder.encode(x_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. Set Data in mipnet\n",
    "for ix, uname in enumerate(user_enc_name):\n",
    "    mip.userEncoding[uname] = user_embs[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Predict interactions from only DOI vs. linear model using DOI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First define the features that are actually being used in current model\n",
    "features = np.array([1, # API from MIP-DOI\n",
    "                     1, # Distance all\n",
    "                     1, # Distance content\n",
    "                     1, # Distance design\n",
    "                     1, # Distance Consume\n",
    "                     1, # Weighted User Connectedness\n",
    "                     1, # Fraction User Connectedness\n",
    "                     1, # Stars / Forks\n",
    "                     1, # User Embeddings\n",
    "                     1, # Owner yes/no\n",
    "                     1, # previous interaction count content\n",
    "                     1, # previous interaction count design\n",
    "                     1, # previous interaction count consume\n",
    "                     1,]) # Total weight between user and users in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 19782 interactions\n"
     ]
    }
   ],
   "source": [
    "# Define Y based on desired outcome, all/design/content/design\n",
    "# Form: {User: {Repo: 1/0}}, possible extension {User: {Repo: {Type: 1/0}}}\n",
    "def get_Y(goaltype='all', time_as_goal=9):\n",
    "    test_ints = get_all_interactions_for_timebucket(time_as_goal, data, label_encodings)\n",
    "    y_truths = defaultdict(Counter)\n",
    "    for p in test_ints:\n",
    "        if goaltype == \"all\" or p[2] == goaltype:\n",
    "            y_truths[p[0]][p[1]] = 1\n",
    "    return y_truths\n",
    "Y = get_Y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user2idx = {}\n",
    "idx2user = {}\n",
    "repo2idx = {}\n",
    "idx2repo = {}\n",
    "for ix, v in enumerate(mip.P):\n",
    "    user2idx[v] = ix\n",
    "    idx2user[ix] = v\n",
    "for ix, v in enumerate(mip.R):\n",
    "    repo2idx[v] = ix\n",
    "    idx2repo[ix] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_large = construct_X_large(Y, mip, mip.mip, mip.P, mip.R, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print X_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "linear1 = Dense(input_shape = (len(X_large[0][0][0][0]),),\n",
    "                units = 50, activation='tanh', kernel_initializer='uniform')\n",
    "linear2 = Dense(2, kernel_initializer='uniform')\n",
    "act = Activation('softmax')\n",
    "\n",
    "model.add(linear1)\n",
    "model.add(linear2)\n",
    "model.add(act)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "Done with Epoch 0\n",
      "0\n",
      "10000\n",
      "Done with Epoch 1\n",
      "0\n",
      "10000\n",
      "Done with Epoch 2\n",
      "0\n",
      "10000\n",
      "Done with Epoch 3\n",
      "0\n",
      "10000\n",
      "Done with Epoch 4\n",
      "0\n",
      "10000\n",
      "Done with Epoch 5\n",
      "0\n",
      "10000\n",
      "Done with Epoch 6\n",
      "0\n",
      "10000\n",
      "Done with Epoch 7\n",
      "0\n",
      "10000\n",
      "Done with Epoch 8\n",
      "0\n",
      "10000\n",
      "Done with Epoch 9\n",
      "0\n",
      "10000\n",
      "Done with Epoch 10\n",
      "0\n",
      "10000\n",
      "Done with Epoch 11\n",
      "0\n",
      "10000\n",
      "Done with Epoch 12\n",
      "0\n",
      "10000\n",
      "Done with Epoch 13\n",
      "0\n",
      "10000\n",
      "Done with Epoch 14\n",
      "0\n",
      "10000\n",
      "Done with Epoch 15\n",
      "0\n",
      "10000\n",
      "Done with Epoch 16\n",
      "0\n",
      "10000\n",
      "Done with Epoch 17\n",
      "0\n",
      "10000\n",
      "Done with Epoch 18\n",
      "0\n",
      "10000\n",
      "Done with Epoch 19\n"
     ]
    }
   ],
   "source": [
    "k = 14\n",
    "epochs = 20\n",
    "\n",
    "total_empty = 0\n",
    "total_data = 0\n",
    "\n",
    "kinds = np.arange(k+1)\n",
    "for ep in range(epochs):\n",
    "    for userid, feat in X_large.iteritems(): \n",
    "        for xtruefeat, xtrueid in feat[0]:\n",
    "            #np.random.shuffle(kinds)\n",
    "            #xtruefeat, xtrueid = feat[0][np.random.choice(len(feat[0]))]\n",
    "            xfalsefeat, xfalseids = zip(*[feat[1][hihi] for hihi in np.random.choice(len(feat[1]), k)])\n",
    "\n",
    "            inputs = np.vstack([np.array(xtruefeat), np.array(xfalsefeat)])\n",
    "            targets = np.array([1,0]+[0,1]*k).reshape(-1,2)\n",
    "            model.train_on_batch(inputs,targets)\n",
    "\n",
    "            #print model.predict(np.array(xtruefeat).reshape(1,-1))\n",
    "\n",
    "            #break\n",
    "        if userid % 5000 == 0:\n",
    "            print userid\n",
    "    print \"Done with Epoch\", ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.829783\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "def evalModel():\n",
    "    ranks = []\n",
    "    top50PerUser = []\n",
    "    now = time.clock()\n",
    "    for userkey, u in idx2user.iteritems(): \n",
    "        try:\n",
    "            _, positive_ids = zip(*X_large[user2idx[u]][0])\n",
    "        except:\n",
    "            # user has no data\n",
    "            positive_ids = []\n",
    "        positive_ids = list(positive_ids)\n",
    "        # Don't even predict if not a single positive is in\n",
    "        if positive_ids:\n",
    "            crank = []\n",
    "            all_preds = []\n",
    "            # Sample 500 other repos, otherwise it will take ~32 hours\n",
    "            look_at = set(np.random.choice(len(idx2repo), 500)) | set(positive_ids)\n",
    "            for k in look_at:\n",
    "                r = idx2repo[k]\n",
    "                currX = get_X_features(mip, mip.mip, u, r, features).reshape(1,-1)\n",
    "                currS = model.predict(currX)[0][0]\n",
    "                curry = 1 if k in positive_ids else 0\n",
    "                all_preds.append((currS, k, userkey, curry))\n",
    "            all_preds.sort(reverse=True)\n",
    "            for predrank, anypred in enumerate(all_preds):\n",
    "                if anypred[1] in positive_ids:\n",
    "                    crank.append(predrank+1)\n",
    "            top50PerUser.append(all_preds[:50])\n",
    "            ranks.append(crank)\n",
    "        if userkey > 1000: \n",
    "            break\n",
    "    print 'Ran Prediction in', time.clock() - now, 'Seconds'\n",
    "    total_ranks = 0\n",
    "    total_within5 = 0\n",
    "    total_within10 = 0\n",
    "    total_within20 = 0\n",
    "    total_num = 0\n",
    "    for ex in ranks:\n",
    "        for num in ex:\n",
    "            total_num += 1\n",
    "            total_ranks += num+1 # remove 1 when recomputed\n",
    "            if num <= 5:\n",
    "                total_within5 += 1\n",
    "            if num <= 10:\n",
    "                total_within10 += 1\n",
    "            if num <= 20:\n",
    "                total_within20 += 1\n",
    "    print total_num, \"Total Number \"\n",
    "    print float(total_within5) / total_num, \"Within 5\"\n",
    "    print float(total_within10) / total_num, \"Within 10\"\n",
    "    print float(total_within20) / total_num, \"Within 20\"\n",
    "    print float(total_ranks)/total_num, \"MRR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "\n",
    "1. Autoencoder Feature per user (Train one until time 8, one until time 9) - DUNZO\n",
    "2. Split Training and Eval sets. Methodology: (1) precompute all inputs until time 8 and train model to predict time 9, (2) update all measures (including autoencoder) and inputset, predict time 10 and eval recall@, MRR\n",
    "3. Implement mask for in training and run experiments\n",
    "4. Evaluate parameters (linear model or saliency?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
